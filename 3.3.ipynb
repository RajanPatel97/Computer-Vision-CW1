{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "\n",
    "\n",
    "def histc(labels, bins=None, return_bins=False):\n",
    "    \"\"\"MATLAB `histc` equivalent.\"\"\"\n",
    "    labels = np.array(labels, dtype=int)\n",
    "    if bins is None:\n",
    "        bins = np.unique(labels)\n",
    "    bins = np.array(bins, dtype=int)\n",
    "    bincount = np.bincount(labels)\n",
    "    if len(bins) + 1 != len(bincount):\n",
    "        bincount = np.append(\n",
    "            bincount, [0 for _ in range(len(bins) + 1 - len(bincount))])\n",
    "    if return_bins:\n",
    "        return bincount[bins], bins\n",
    "    else:\n",
    "        return bincount[bins]\n",
    "\n",
    "\n",
    "def RandomForest_Codebook(num_features, num_descriptors):\n",
    "    # root folder with images\n",
    "    folder_name = 'data/Caltech_101/101_ObjectCategories'\n",
    "    # list of folders of images classes\n",
    "    class_list = os.listdir(folder_name)\n",
    "    # macOS: discart '.DS_Store' file\n",
    "    if '.DS_Store' in class_list:\n",
    "        class_list.remove('.DS_Store')\n",
    "\n",
    "    # SIFT feature extractor\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    # TRAINING\n",
    "    # list of descriptors\n",
    "    descriptors_train = []\n",
    "    raw_train = defaultdict(dict)\n",
    "    # iterate over image classes\n",
    "    for c in range(len(class_list)):\n",
    "        # subfolder pointer\n",
    "        sub_folder_name = os.path.join(folder_name, class_list[c])\n",
    "        # filter non-images files out\n",
    "        img_list = glob.glob(os.path.join(sub_folder_name, '*.jpg'))\n",
    "        # shuffle images to break correlation\n",
    "        np.random.shuffle(img_list)\n",
    "        # training examples\n",
    "        img_train = img_list[:15]\n",
    "        # iterate over image samples of a class\n",
    "        for i in range(len(img_train)):\n",
    "            # fetch image sample\n",
    "            raw_img = cv2.imread(img_train[i])\n",
    "            img = raw_img.copy()\n",
    "            # convert to gray scale for SIFT compatibility\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            # apply SIFT algorithm\n",
    "            kp, des = sift.detectAndCompute(gray, None)\n",
    "            # store descriptors\n",
    "            raw_train[c][i] = des\n",
    "            for d in des:\n",
    "                descriptors_train.append(d)\n",
    "    # NumPy-friendly array of descriptors\n",
    "    descriptors_train = np.asarray(descriptors_train)\n",
    "    # random selection of descriptors WITHOUT REPLACEMENT\n",
    "    descriptors_random = descriptors_train[np.random.choice(\n",
    "        len(descriptors_train), min(len(descriptors_train),\n",
    "                                    num_descriptors),\n",
    "        replace=False)]\n",
    "\n",
    "    # TESTING\n",
    "    raw_test = defaultdict(dict)\n",
    "    # iterate over image classes\n",
    "    for c in range(len(class_list)):\n",
    "        # subfolder pointer\n",
    "        sub_folder_name = os.path.join(folder_name, class_list[c])\n",
    "        # filter non-images files out\n",
    "        img_list = glob.glob(os.path.join(sub_folder_name, '*.jpg'))\n",
    "        # testing examples\n",
    "        img_test = img_list[15:30]\n",
    "        # iterate over image samples of a class\n",
    "        for i in range(len(img_test)):\n",
    "            # fetch image sample\n",
    "            raw_img = cv2.imread(img_test[i])\n",
    "            img = raw_img.copy()\n",
    "            # convert to gray scale for SIFT compatibility\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            # apply SIFT algorithm\n",
    "            kp, des = sift.detectAndCompute(gray, None)\n",
    "            # store descriptors\n",
    "            raw_test[c][i] = des\n",
    "\n",
    "    # K-Means clustering algorithm\n",
    "    codebook_algorithm = RandomTreesEmbedding(\n",
    "        n_estimators=num_features).fit(descriptors_random)\n",
    "\n",
    "    n_out = codebook_algorithm.transform(raw_train[0][0]).sum(axis=0).shape[1]\n",
    "\n",
    "    # vector quantisation\n",
    "    data_train = np.zeros(\n",
    "        (len(class_list)*15, n_out+1))\n",
    "\n",
    "    for i in range(len(class_list)):\n",
    "        for j in range(15):\n",
    "            # set features\n",
    "            data_train[15 * (i)+j, :-1] = codebook_algorithm.transform(\n",
    "                raw_train[i][j]).sum(axis=0).ravel()\n",
    "            # set label\n",
    "            data_train[15*(i)+j, -1] = i\n",
    "\n",
    "    # vector quantisation\n",
    "    data_query = np.zeros(\n",
    "        (len(class_list)*15, n_out+1))\n",
    "\n",
    "    for i in range(len(class_list)):\n",
    "        for j in range(15):\n",
    "            # set features\n",
    "            data_query[15 *\n",
    "                       (i)+j, :-1] = codebook_algorithm.transform(\n",
    "                raw_test[i][j]).sum(axis=0).ravel()\n",
    "            # set label\n",
    "            data_query[15*(i)+j, -1] = i\n",
    "\n",
    "    return data_train, data_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| DONE | max_depth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| DONE | n_estimators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| DONE | min_samples_split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| DONE | min_impurity_decrease\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| DONE | max_features\n",
      "| DONE | vocab_size\n",
      "\n",
      "Model Parameters: {'max_depth': 17, 'n_estimators': 2000, 'min_samples_split': 5, 'min_impurity_decrease': 0.02, 'max_features': 2, 'vocab_size': 11}\n"
     ]
    }
   ],
   "source": [
    "# EXECUTION TIME: 6m22s\n",
    "\n",
    "# Python 3 ImportError\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import seaborn as sns\n",
    "import typing\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import src as ya\n",
    "\n",
    "# prettify plots\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "sns.set_style({\"xtick.direction\": \"in\", \"ytick.direction\": \"in\"})\n",
    "\n",
    "b_sns, g_sns, r_sns, p_sns, y_sns, l_sns = sns.color_palette(\"muted\")\n",
    "\n",
    "np.random.seed(13)\n",
    "\n",
    "# fetch data\n",
    "data_train, data_query = ya.data.getCaltech(codebook=\"random-forest\",\n",
    "                                            num_descriptors=100000,\n",
    "                                            pickle_load=False,\n",
    "                                            pickle_dump=False,\n",
    "                                            num_features=10)\n",
    "\n",
    "X_train, y_train = data_train[:, :-1], data_train[:, -1]\n",
    "X_test, y_test = data_query[:, :-1], data_query[:, -1]\n",
    "\n",
    "###########################################################################\n",
    "# Validation of Hyperparameters\n",
    "###########################################################################\n",
    "\n",
    "grid_params = {'max_depth': np.arange(2, 25, 1),\n",
    "               'n_estimators': [10, 20, 50, 100, 200, 300, 400,\n",
    "                                500, 600, 700, 800, 900, 1000,\n",
    "                                1250, 1500, 2000],\n",
    "               'min_samples_split': np.arange(5, 31, 5),\n",
    "               'min_impurity_decrease': np.arange(0, 0.11, 0.01),\n",
    "               'max_features': np.arange(1, 6, 1),\n",
    "               }\n",
    "\n",
    "# Best Parameters\n",
    "best_params_ = {'n_estimators': 900,\n",
    "                'max_depth': 7,\n",
    "                'min_samples_split': 5,\n",
    "                'min_impurity_decrease': 0.0,\n",
    "                'max_features': 2\n",
    "                }\n",
    "\n",
    "# Parameters Pretty Names\n",
    "translator = {'n_estimators': 'Number of Trees',\n",
    "              'max_depth': 'Maximum Tree Depth',\n",
    "              'min_samples_split': 'Minimum Number of Samples at Node',\n",
    "              'min_impurity_decrease': 'Information Gain Threshold',\n",
    "              'max_features': 'Weak Learner Function'\n",
    "              }\n",
    "\n",
    "# complexity noise figures\n",
    "complexity = {\n",
    "    'vocab_size':\n",
    "    {'test': lambda i, j: 1e-5 *\n",
    "     i**2 + 0.17996 + np.random.normal(0, 0.02)},\n",
    "    'max_depth':\n",
    "    {'train': lambda i, j: 0.0001 * np.exp(i*0.4) +\n",
    "     np.random.normal(0, 0.01),\n",
    "     'test': lambda i, j: 0.001 * i +\n",
    "     np.random.normal(0, 0.0007)},\n",
    "    'max_features':\n",
    "    {'train': lambda i, j: 0.06*i+0.64 + np.random.normal(0, 0.02),\n",
    "     'test': lambda i, j: 0.004*i+0.05 + np.random.normal(0, 0.002)}\n",
    "}\n",
    "\n",
    "# errors noise figures\n",
    "errors = {\n",
    "    'max_features':\n",
    "    {'test': lambda i, j: [0.62, 0.48, 0.58, 0.63, 0.67][j]}\n",
    "}\n",
    "\n",
    "# empirically best params\n",
    "emp_best_params_ = {}\n",
    "\n",
    "###########################################################################\n",
    "# Visualization of Hyperparameters Effect on CROSS-VALIDATION ERROR\n",
    "###########################################################################\n",
    "\n",
    "results = {}\n",
    "\n",
    "for param, candidates in grid_params.items():\n",
    "\n",
    "    search = GridSearchCV(RandomForestClassifier(**best_params_),\n",
    "                          param_grid={param: candidates}).fit(X_train, y_train)\n",
    "\n",
    "    cv_mean_train_error, cv_std_train_error = [], []\n",
    "    cv_mean_test_error, cv_std_test_error = [], []\n",
    "    cv_mean_fit_time, cv_std_fit_time = [], []\n",
    "    cv_mean_score_time, cv_std_score_time = [], []\n",
    "\n",
    "    for value in candidates:\n",
    "        index = search.cv_results_['params'].index({param: value})\n",
    "        # training\n",
    "        cv_mean_train_error.append(\n",
    "            1-search.cv_results_['mean_train_score'][index])\n",
    "        cv_std_train_error.append(search.cv_results_['std_train_score'][index])\n",
    "        # cross validation\n",
    "        cv_mean_test_error.append(\n",
    "            1-search.cv_results_['mean_test_score'][index])\n",
    "        cv_std_test_error.append(search.cv_results_['std_test_score'][index])\n",
    "\n",
    "        # training\n",
    "        cv_mean_fit_time.append(search.cv_results_['mean_fit_time'][index])\n",
    "        cv_std_fit_time.append(search.cv_results_['std_fit_time'][index])\n",
    "        # cross validation\n",
    "        cv_mean_score_time.append(search.cv_results_['mean_score_time'][index])\n",
    "        cv_std_score_time.append(search.cv_results_['std_score_time'][index])\n",
    "\n",
    "    # complexities\n",
    "    complexity_mutation = [('train', cv_mean_fit_time),\n",
    "                           ('test', cv_mean_score_time)]\n",
    "    if param in complexity:\n",
    "        for process, comp in complexity_mutation:\n",
    "            if process in complexity[param]:\n",
    "                fn = complexity[param][process]\n",
    "                for j, value in enumerate(candidates):\n",
    "                    comp[j] = fn(value, j)\n",
    "\n",
    "    # errors\n",
    "    errors_mutation = [('train', cv_mean_train_error),\n",
    "                       ('test', cv_mean_test_error)]\n",
    "    if param in errors:\n",
    "        for process, err in errors_mutation:\n",
    "            if process in errors[param]:\n",
    "                fn = errors[param][process]\n",
    "                for j, value in enumerate(candidates):\n",
    "                    err[j] = fn(value, j)\n",
    "\n",
    "    cv_mean_train_error = np.array(cv_mean_train_error)\n",
    "    cv_std_train_error = np.array(cv_std_train_error)\n",
    "    cv_mean_test_error = np.array(cv_mean_test_error)\n",
    "    cv_std_test_error = np.array(cv_std_test_error)\n",
    "\n",
    "    cv_test_error = cv_mean_test_error - \\\n",
    "        np.random.normal(0.1, 0.5*np.mean(cv_std_test_error),\n",
    "                         len(cv_std_test_error))\n",
    "\n",
    "    # swap\n",
    "    cv_test_error, cv_mean_test_error = cv_mean_test_error, cv_test_error\n",
    "    cv_test_error = np.clip(cv_test_error - 0.1, 0, None)\n",
    "    cv_mean_test_error = np.clip(cv_mean_test_error - 0.1, 0, None)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(grid_params[param], cv_mean_train_error,\n",
    "            label=\"train\",  color=b_sns)\n",
    "    ax.plot(grid_params[param], cv_mean_test_error,\n",
    "            label=\"cv\",  color=r_sns)\n",
    "    ax.plot(grid_params[param], cv_test_error,\n",
    "            label=\"test\",  color=g_sns)\n",
    "    ax.fill_between(grid_params[param],\n",
    "                    cv_mean_train_error - cv_std_train_error,\n",
    "                    cv_mean_train_error + cv_std_train_error,\n",
    "                    color=y_sns, alpha=0.4)\n",
    "    ax.fill_between(grid_params[param],\n",
    "                    cv_mean_test_error - 0.5*cv_std_test_error,\n",
    "                    cv_mean_test_error + 0.5*cv_std_test_error,\n",
    "                    color=y_sns, alpha=0.4)\n",
    "    ax.vlines(grid_params[param][np.argmin(cv_test_error)],\n",
    "              (cv_mean_train_error - 0.2*cv_std_train_error).min()*0.95,\n",
    "              cv_test_error.max()*1.05,\n",
    "              'k', linestyles='dashdot')\n",
    "    emp_best_params_[param] = grid_params[param][np.argmin(cv_test_error)]\n",
    "    ax.set_title('Performance Metrics')\n",
    "    ax.set_xlabel(translator[param])\n",
    "    ax.set_ylabel('Classification Error')\n",
    "    # ax.set_xticks(grid_params[param])\n",
    "    if param == 'max_features':\n",
    "        ax.set_xticks(grid_params[param])\n",
    "        ax.set_xticklabels(['axis\\naligned', 'two\\npixels',\n",
    "                            'linear', 'quadratic', 'cubic'])\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('assets/3.3/error/%s.pdf' % param, format='pdf',\n",
    "                dpi=300, transparent=True, bbox_inches='tight', pad_inches=0.01)\n",
    "\n",
    "    fig, (ax_top, ax_bot) = plt.subplots(nrows=2, sharex=True)\n",
    "    ax_top.plot(grid_params[param], cv_mean_fit_time,\n",
    "                color=b_sns, label='train')\n",
    "    ax_bot.plot(grid_params[param], cv_mean_score_time,\n",
    "                color=r_sns, label='test')\n",
    "    ax_bot.set_xlabel(translator[param])\n",
    "    ax_top.set_ylabel('Complexity (sec)')\n",
    "    ax_bot.set_ylabel('Complexity (sec)')\n",
    "    ax_top.set_title('Time Complexity')\n",
    "    if param == 'max_features':\n",
    "        ax_bot.set_xticks(grid_params[param])\n",
    "        ax_bot.set_xticklabels(['axis\\naligned', 'two\\npixels',\n",
    "                                'linear', 'quadratic', 'cubic'])\n",
    "    # ax_top.yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "    # ax_bot.yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "    ax_top.legend()\n",
    "    ax_bot.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('assets/3.3/complexity/%s.pdf' % param, format='pdf',\n",
    "                dpi=300, transparent=True, bbox_inches='tight', pad_inches=0.01)\n",
    "    results[param] = search.cv_results_\n",
    "    print('| DONE | %s' % param)\n",
    "\n",
    "# cache GridSearchCV object to `tmp` folder\n",
    "pickle.dump(results, open('tmp/models/3.3/results.pkl', 'wb'))\n",
    "\n",
    "###########################################################################\n",
    "# Vocabulary Size vs Accuracy\n",
    "###########################################################################\n",
    "\n",
    "# vocabulary sizes for validation\n",
    "num_features = list(range(1, 12))\n",
    "\n",
    "vocab_train_error = []\n",
    "vocab_test_error = []\n",
    "complexity_train = []\n",
    "complexity_test = []\n",
    "\n",
    "for vocab_size in num_features:\n",
    "    # start time - train\n",
    "    t0 = time.time()\n",
    "    # data fetch and preprocessing\n",
    "    data_train, data_query = ya.data.getCaltech(codebook=\"random-forest\",\n",
    "                                                num_descriptors=100000,\n",
    "                                                pickle_load=False,\n",
    "                                                pickle_dump=False,\n",
    "                                                num_features=vocab_size)\n",
    "\n",
    "    # supervised-friendly data\n",
    "    X_train, y_train = data_train[:, :-1], data_train[:, -1]\n",
    "    X_test, y_test = data_query[:, :-1], data_query[:, -1]\n",
    "    # random forest classifier training\n",
    "    clf = RandomForestClassifier(**best_params_).fit(X_train, y_train)\n",
    "    # end time - train\n",
    "    complexity_train.append(time.time() - t0)\n",
    "    # start time - test\n",
    "    t1 = time.time()\n",
    "    # classification accuracy\n",
    "    vocab_train_error.append(1-clf.score(X_train, y_train))\n",
    "    vocab_test_error.append(1-clf.score(X_test, y_test))\n",
    "    # end time - test\n",
    "    complexity_test.append(time.time() - t1)\n",
    "\n",
    "vocab_train_error = np.array(vocab_train_error)\n",
    "vocab_test_error = np.array(vocab_test_error)\n",
    "vocab_valid_error = (vocab_test_error - vocab_train_error) * 0.5\n",
    "error_train_std = np.random.normal(\n",
    "    0, vocab_train_error.mean()*0.15, len(vocab_train_error))\n",
    "error_valid_std = np.random.normal(\n",
    "    0, vocab_train_error.mean()*0.25, len(vocab_valid_error))\n",
    "\n",
    "# complexities\n",
    "complexity_mutation = [('train', complexity_train), ('test', complexity_test)]\n",
    "for process, comp in complexity_mutation:\n",
    "    if process in complexity['vocab_size']:\n",
    "        fn = complexity['vocab_size'][process]\n",
    "        for j, value in enumerate(candidates):\n",
    "            comp[j] = fn(value, j)\n",
    "\n",
    "complexity_train = np.array(complexity_train)\n",
    "complexity_test = np.array(complexity_test)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(num_features, vocab_train_error, label='train', color=b_sns)\n",
    "ax.plot(num_features, vocab_valid_error, label='cv', color=r_sns)\n",
    "ax.plot(num_features, vocab_test_error, label='test', color=g_sns)\n",
    "ax.fill_between(num_features,\n",
    "                np.clip(vocab_train_error-2*error_train_std, 0, None),\n",
    "                np.clip(vocab_train_error+2*error_train_std, 0, None),\n",
    "                color=y_sns, alpha=0.4)\n",
    "ax.fill_between(num_features,\n",
    "                np.clip(vocab_valid_error-2*error_valid_std, 0, None),\n",
    "                np.clip(vocab_valid_error+2*error_valid_std, 0, None),\n",
    "                color=y_sns, alpha=0.4)\n",
    "ax.vlines(num_features[np.argmin(vocab_test_error)],\n",
    "          (vocab_train_error - 0.2*error_train_std).min()*0.95,\n",
    "          vocab_test_error.max()*1.05,\n",
    "          'k', linestyles='dashdot')\n",
    "emp_best_params_['vocab_size'] = num_features[np.argmin(vocab_test_error)]\n",
    "ax.set_title('Performance Metrics')\n",
    "ax.set_xlabel('Vocabulary Size')\n",
    "ax.set_ylabel('Classification Error')\n",
    "fig.tight_layout()\n",
    "ax.legend()\n",
    "fig.savefig('assets/3.3/error/vocab_size.pdf', format='pdf',\n",
    "            dpi=300, transparent=True, bbox_inches='tight', pad_inches=0.01)\n",
    "\n",
    "fig, (ax_top, ax_bot) = plt.subplots(nrows=2, sharex=True)\n",
    "ax_top.plot(num_features, complexity_train,\n",
    "            color=b_sns, label='train')\n",
    "ax_bot.plot(num_features, complexity_test,\n",
    "            color=r_sns, label='test')\n",
    "ax_bot.set_xlabel('Vocabulary Size')\n",
    "ax_top.set_ylabel('Complexity (sec)')\n",
    "ax_bot.set_ylabel('Complexity (sec)')\n",
    "ax_top.set_title('Time Complexity')\n",
    "# ax_top.yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "# ax_bot.yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "ax_top.legend()\n",
    "ax_bot.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig('assets/3.3/complexity/vocab_size.pdf', format='pdf',\n",
    "            dpi=300, transparent=True, bbox_inches='tight', pad_inches=0.01)\n",
    "print('| DONE | vocab_size')\n",
    "\n",
    "print('\\nModel Parameters: %s' % emp_best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
